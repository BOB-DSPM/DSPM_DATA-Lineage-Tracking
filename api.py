from __future__ import annotations
from typing import Optional, List, Dict, Any, Tuple
from botocore.config import Config
from botocore.exceptions import ClientError
from fastapi import FastAPI, HTTPException, Query, Depends, Request
from fastapi.middleware.cors import CORSMiddleware
from pathlib import Path
from pydantic import BaseModel
import os
import boto3
import uvicorn
import shutil
from urllib.parse import urljoin
import asyncio
import httpx, re

# --- Internal modules ---
from modules.schema_sampler import sample_schema, parse_s3_uri
from modules.schema_store import save_schema, dataset_id_from_s3, get_version, list_versions
from modules.featurestore_schema import describe_feature_group, list_feature_groups
from modules.sql_collector import collect_from_repo
from modules.sql_lineage_store import put, get_by_pipeline, get_by_job
from modules.connectors.git_fetch import shallow_clone
from modules.sql_try import try_parse

# ìˆœí™˜ import ë°©ì§€ìš© ë³„ì¹­ ì„í¬íŠ¸
import lineage as lineage_lib

# -----------------------------------------------------------------------------#
# FastAPI
# -----------------------------------------------------------------------------#
app = FastAPI(title="SageMaker Lineage API", version="1.5.1")

# CORS (ìš´ì˜ ì‹œ íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ ê¶Œì¥)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],      # â† POST/OPTIONS ëª¨ë‘ í—ˆìš©
    allow_headers=["*"],
)

# -----------------------------------------------------------------------------#
# boto3 ê³µí†µ ì„¤ì •
# -----------------------------------------------------------------------------#
_BOTO_CFG = Config(
    retries={"max_attempts": 5, "mode": "adaptive"},
    connect_timeout=5,
    read_timeout=60,
)

# -----------------------------------------------------------------------------#
# Utils
# -----------------------------------------------------------------------------#
S3_RE = re.compile(r"^s3://([^/]+)/?(.*)$")

def parse_s3_uri(uri: str) -> Tuple[str, str]:
    """
    s3://bucket/prefix -> (bucket, prefix)
    prefixê°€ ì—†ìœ¼ë©´ '' ë°˜í™˜
    """
    m = S3_RE.match(uri or "")
    if not m:
        raise ValueError(f"Invalid S3 URI: {uri}")
    bucket, prefix = m.group(1), m.group(2)
    return bucket, prefix

def data_node_id_from_uri(uri: str) -> str:
    # í”„ë¡ íŠ¸ ë°ì´í„° ë…¸ë“œ id ê·œì¹™ì— ë§ì¶° í†µì¼
    return f"data:{uri}"

def is_data_uri(uri: str) -> bool:
    """
    ì½”ë“œ/ëª¨ë¸ íŒŒì¼ ë“± ë°ì´í„°ê°€ ì•„ë‹Œ ëŒ€ìƒì€ ì œì™¸
    """
    if not isinstance(uri, str):
        return False
    if not uri.startswith("s3://"):
        return False
    lower = uri.lower()
    if lower.endswith(".py") or lower.endswith(".ipynb") or lower.endswith(".tar.gz") or lower.endswith(".model"):
        return False
    return True

def _parse_regions(regions: Optional[str], profile: Optional[str]) -> List[str]:
    """regions ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì—†ìœ¼ë©´ SageMaker ì§€ì› ëª¨ë“  ë¦¬ì „ ë°˜í™˜"""
    if regions:
        return [r.strip() for r in regions.split(",") if r.strip()]
    sess = boto3.session.Session(profile_name=profile) if profile else boto3.session.Session()
    return sess.get_available_regions("sagemaker")

def _get_latest_pipeline_execution(sm, pipeline_name: str) -> Dict[str, Any]:
    """ìµœì‹  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ 1ê±´ ìš”ì•½"""
    resp = sm.list_pipeline_executions(
        PipelineName=pipeline_name,
        SortBy="CreationTime",
        SortOrder="Descending",
        MaxResults=1
    )
    exes = resp.get("PipelineExecutionSummaries", [])
    if not exes:
        return {}
    x = exes[0]
    return {
        "arn": x.get("PipelineExecutionArn"),
        "status": x.get("PipelineExecutionStatus"),
        "startTime": x.get("StartTime").isoformat() if x.get("StartTime") else None,
        "lastModifiedTime": x.get("LastUpdatedTime").isoformat() if x.get("LastUpdatedTime") else None
    }

def guess_step_from_path(path: str, pipeline: str) -> str:
    import re, os as _os
    base = _os.path.basename(path or "")
    m = re.match(r"(\d+_)?([a-zA-Z0-9\-_]+)", base)
    step = (m.group(2) if m else base).replace(".sql", "").replace(".py", "")
    return step

async def fetch_schema_layer(
    request: Request,
    pipeline: str,
    region: str,
    include_featurestore: bool = True,
    include_sql: bool = True,
    scan_if_missing: bool = False,   # ìŠ¤í‚¤ë§ˆ ì—†ìœ¼ë©´ ìë™ ìŠ¤ìº”
    timeout_s: float = 15.0,
) -> Dict[str, Any]:
    """
    /lineage/schema ì§‘ê³„ ë¡œì§:
      - /lineage â†’ ë°ì´í„° ê´€ì  ê·¸ë˜í”„ì—ì„œ dataArtifact ë…¸ë“œ ì¶”ì¶œ
      - ê° S3 URIì— ëŒ€í•´ /datasets/{bucket}/{prefix}/schema í˜¸ì¶œ
      - (ì˜µì…˜) FeatureStore, SQL ë¼ì¸ë¦¬ì§€ ë³´ê°•
      - í”„ë¡ íŠ¸ê°€ ë°”ë¡œ ì“°ëŠ” tables/columns/featureGroups/features + links ë°˜í™˜
    """
    base = str(request.base_url).rstrip("/")
    warnings: List[str] = []

    async with httpx.AsyncClient(base_url=base, timeout=timeout_s) as client:
        # 1) ë¼ì¸ë¦¬ì§€(graphData í™•ë³´)
        r = await client.get("/lineage", params={"pipeline": pipeline, "region": region, "view": "data"})
        if r.status_code != 200:
            raise HTTPException(status_code=502, detail=f"lineage fetch failed: {r.text}")
        lineage = r.json() or {}
        data_graph = lineage.get("graphData") or {}
        nodes = data_graph.get("nodes") or []

        # 2) dataArtifact -> S3 URI ë§¤í•‘
        artifact_map: Dict[str, str] = {}  # nodeId -> s3://...
        for dn in (n for n in nodes if n.get("type") == "dataArtifact"):
            node_id = dn.get("id") or ""
            
            # id ê·œì¹™ì´ data:s3://... ì¸ ì¼€ì´ìŠ¤
            if node_id.startswith("data:s3://"):
                s3 = node_id[5:]
                if is_data_uri(s3):
                    artifact_map[node_id] = s3
                    print(f"[fetch_schema_layer] Mapped from node_id: {node_id} -> {s3}")
                continue

            # uri í•„ë“œì—ì„œ ì¶”ì¶œ
            uri = dn.get("uri")
            if is_data_uri(uri):
                mapped_id = data_node_id_from_uri(uri)
                artifact_map[data_node_id_from_uri(uri)] = uri
                print(f"[fetch_schema_layer] Mapped from uri: {mapped_id} -> {uri}")

        # í›„ë³´ URI ì •ë¦¬
        uris = sorted(set(artifact_map.values()))
        print(f"[fetch_schema_layer] Total unique URIs: {len(uris)}")
        for uri in uris:
            print(f"  - {uri}")

        # URIê°€ ì—†ì–´ë„ ê³„ì† ì§„í–‰ (SQL ë¼ì¸ë¦¬ì§€ì—ì„œ í…Œì´ë¸” ì •ë³´ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ)
        if not uris:
            warnings.append("no data artifacts found in lineage graph")

        # (ì„ íƒ) ìŠ¤ìº” íŠ¸ë¦¬ê±°
        if scan_if_missing and uris:
            for u in uris:
                try:
                    await client.post(
                        "/datasets/schema/scan",
                        params={"region": region, "s3_uri": u},
                    )
                except Exception as e:
                    warnings.append(f"schema scan failed for {u}: {e}")

        # 3) ìŠ¤í‚¤ë§ˆ fetch with fallback(latest version)
        async def fetch_dataset_schema(uri: str) -> Dict[str, Any]:
            try:
                bucket, prefix = parse_s3_uri(uri)
                parts = [p for p in prefix.split("/") if p]
                tried = []

                # uriì—ì„œ ìƒìœ„ í´ë”ë¡œ í•œ ë‹¨ê³„ì”© ì˜¬ë¼ê°€ë©° /schema ì¡°íšŒ
                for i in range(len(parts), 0, -1):
                    candidate = "/".join(parts[:i])
                    tried.append(candidate)
                    res = await client.get(f"/datasets/{bucket}/{candidate}/schema")
                    if res.status_code == 200:
                        data = res.json() or {}
                        # ì–´ë–¤ prefixì— ë§¤ì¹­ëëŠ”ì§€ ê°™ì´ ë°˜í™˜
                        return {
                            "uri": uri,
                            "ok": True,
                            "data": data,
                            "bucket": bucket,
                            "matched_prefix": candidate,
                        }

                # ë§ˆì§€ë§‰ìœ¼ë¡œ ì œì¼ ìƒìœ„ prefixë„ ì•ˆ ë˜ë©´ ì‹¤íŒ¨
                return {"uri": uri, "ok": False, "error": f"no schema for {tried}"}

            except Exception as e:
                return {"uri": uri, "ok": False, "error": str(e)}

        dataset_results = []
        if uris:
            dataset_results = await asyncio.gather(*(fetch_dataset_schema(u) for u in uris))

        # 4) normalize -> tables/columns
        tables: List[Dict[str, Any]] = []
        columns: List[Dict[str, Any]] = []

        for res in dataset_results:
            if not res["ok"]:
                warnings.append(f"{res['uri']}: {res.get('error')}")
                continue

            data = res["data"] or {}
            bucket = res["bucket"]
            matched_prefix = res["matched_prefix"]

            # dataset_id ì—†ìœ¼ë©´ matched_prefixë¡œ êµ¬ì„±
            dataset_id = (
                data.get("dataset_id")
                or data.get("id")
                or f"s3://{bucket}/{matched_prefix}"
            )

            # table ì´ë¦„: dataset_id ë§ˆì§€ë§‰ í† í° (pipelines::exp1 -> exp1)
            raw_name = dataset_id.split("::")[-1]
            t_name = raw_name.rstrip("/").split("/")[-1]
            t_id = f"table:{t_name}"

            # ì´ ìŠ¤í‚¤ë§ˆê°€ ì»¤ë²„í•˜ëŠ” data ë…¸ë“œë“¤ê³¼ ì—°ê²°
            links: List[str] = []
            prefix_uri = f"s3://{bucket}/{matched_prefix}"
            for node_id, s3 in artifact_map.items():
                if s3.startswith(prefix_uri):
                    links.append(node_id)

            links = sorted(set(links))
            
            # linksê°€ ì—†ì–´ë„ í…Œì´ë¸”ì€ ì¶”ê°€ (í”„ë¡ íŠ¸ì— í‘œì‹œí•˜ê¸° ìœ„í•´)
            tables.append({
                "id": t_id,
                "name": t_name,
                "version": data.get("version"),
                "links": links,
                "s3_prefix": matched_prefix,  # ë””ë²„ê¹…ìš©
            })

            # ----- ì»¬ëŸ¼ ì¶”ì¶œ -----
            # 1) columns ë°°ì—´ í˜•ì‹ì´ ìˆìœ¼ë©´ ìš°ì„ 
            raw_cols = data.get("columns")

            # 2) schema.fields í˜•ì‹ íŒŒì‹±
            if not raw_cols and isinstance(data.get("schema"), dict):
                fields = data["schema"].get("fields") or {}
                raw_cols = []
                for cname, meta in fields.items():
                    # í•„ìš” ì—†ìœ¼ë©´ ìƒ˜í”Œ/ë©”íƒ€ í•„ë“œ ì œì™¸í•  ìˆ˜ë„ ìˆìŒ
                    if cname in ("sampled_files",):
                        continue
                    ctype = None
                    if isinstance(meta, dict):
                        ts = meta.get("types") or meta.get("type")
                        if isinstance(ts, list):
                            ctype = " | ".join(str(t) for t in ts)
                        else:
                            ctype = ts
                    raw_cols.append({"name": cname, "type": ctype})

            for c in raw_cols or []:
                cname = c.get("name")
                if not cname:
                    continue
                ctype = c.get("type")
                columns.append({
                    "id": f"column:{t_name}.{cname}",
                    "name": cname,
                    "tableId": t_id,
                    "type": ctype,
                    "links": links,
                })

        # 5) SQL ë¼ì¸ë¦¬ì§€ë¡œ í…Œì´ë¸” ë³´ê°• (ìŠ¤í‚¤ë§ˆê°€ ì—†ì„ ë•Œ ì¤‘ìš”!)
        if include_sql:
            try:
                sqlr = await client.get(f"/pipelines/{pipeline}/sql-lineage", params={"region": region})
                if sqlr.status_code == 200:
                    sql = sqlr.json() or {}
                    sql_tables = sql.get("steps", [])
                    
                    # ğŸ”¥ SQLì—ì„œ ë°œê²¬ëœ í…Œì´ë¸” ì¶”ê°€
                    for step in sql_tables:
                        dst = step.get("dst")
                        if not dst:
                            continue
                        
                        t_name = dst.split(".")[-1]  # schema.table -> table
                        t_id = f"table:{t_name}"
                        
                        # ì´ë¯¸ ì¶”ê°€ëœ í…Œì´ë¸”ì¸ì§€ í™•ì¸
                        if any(t["id"] == t_id for t in tables):
                            continue
                        
                        # SQLì—ì„œë§Œ ë°œê²¬ëœ í…Œì´ë¸” ì¶”ê°€
                        tables.append({
                            "id": t_id,
                            "name": t_name,
                            "version": None,
                            "links": [],
                            "source": "sql",  # SQLì—ì„œ ì˜¨ ì •ë³´ì„ì„ í‘œì‹œ
                            "step": step.get("step"),
                        })
                        
                        # ì»¬ëŸ¼ ì •ë³´ë„ ì¶”ê°€
                        for col_name in (step.get("columns") or []):
                            if not col_name:
                                continue
                            columns.append({
                                "id": f"column:{t_name}.{col_name}",
                                "name": col_name,
                                "tableId": t_id,
                                "type": "unknown",
                                "links": [],
                                "source": "sql",
                            })
                    
                    # ê¸°ì¡´ í…Œì´ë¸” ì •ë³´ ë³´ê°•
                    sql_tables_map = { t.get("dst"): t for t in sql_tables if t.get("dst") }
                    for t in tables:
                        if t.get("source") == "sql":
                            continue
                        st = sql_tables_map.get(t["name"])
                        if st:
                            t["sql_step"] = st.get("step")
                            t["sql_file"] = st.get("file")
                            
            except Exception as e:
                warnings.append(f"sql lineage: {e}")

        # 6) Feature Store
        feature_groups: List[Dict[str, Any]] = []
        features: List[Dict[str, Any]] = []
        if include_featurestore:
            try:
                fg_list = await client.get("/featurestore/feature-groups", params={"region": region})
                if fg_list.status_code == 200:
                    for fg in (fg_list.json().get("items", []) or []):
                        name = fg.get("FeatureGroupName") or fg.get("name")
                        if not name:
                            continue
                        det = await client.get(f"/featurestore/feature-groups/{name}", params={"region": region})
                        if det.status_code != 200:
                            continue
                        detj = det.json() or {}
                        s3_uri = (detj.get("OfflineStoreConfig") or {}).get("S3StorageConfig", {}).get("ResolvedOutputS3Uri")
                        links = [data_node_id_from_uri(s3_uri)] if is_data_uri(s3_uri) else []
                        fg_id = f"featureGroup:{name}"
                        feature_groups.append({
                            "id": fg_id,
                            "name": name,
                            "version": detj.get("Version") or detj.get("FeatureGroupVersion"),
                            "links": links,
                        })
                        for f in (detj.get("FeatureDefinitions") or detj.get("Features") or []):
                            fname = f.get("FeatureName") or f.get("name")
                            ftype = f.get("FeatureType") or f.get("type")
                            if not fname:
                                continue
                            features.append({
                                "id": f"feature:{name}.{fname}",
                                "name": fname,
                                "groupId": fg_id,
                                "type": ftype,
                                "links": links,
                            })
            except Exception as e:
                warnings.append(f"feature store: {e}")

        # id ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
        tables = list({t["id"]: t for t in tables}.values())
        columns = list({c["id"]: c for c in columns}.values())
        
        # ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
        print(f"[schema] Pipeline: {pipeline}, Tables: {len(tables)}, Columns: {len(columns)}")
        if tables:
            print(f"[schema] Table names: {[t['name'] for t in tables]}")

        return {
            "tables": tables,
            "columns": columns,
            "featureGroups": feature_groups,
            "features": features,
            "warnings": warnings,
        }

# -----------------------------------------------------------------------------#
# 0) Health
# -----------------------------------------------------------------------------#
@app.get("/health")
def health():
    return {"status": "ok", "version": app.version}

# -----------------------------------------------------------------------------#
# 1) pipelines: íŒŒì´í”„ë¼ì¸ ëª©ë¡ + íƒœê·¸/ë„ë©”ì¸ ë§¤í•‘
# -----------------------------------------------------------------------------#
@app.get("/sagemaker/pipelines")
def sagemaker_pipelines(
    regions: Optional[str] = Query(None, description="ì‰¼í‘œêµ¬ë¶„ ë¦¬ì „ ëª©ë¡. ì—†ìœ¼ë©´ SageMaker ì§€ì› ë¦¬ì „ ì „ì²´"),
    includeLatestExec: bool = Query(False, description="íŒŒì´í”„ë¼ì¸ë³„ ìµœì‹  ì‹¤í–‰ ìš”ì•½ í¬í•¨"),
    profile: Optional[str] = Query(None, description="(ê°œë°œ/ë¡œì»¬) AWS í”„ë¡œí•„ëª…"),
    name: Optional[str] = Query(None, description="íŒŒì´í”„ë¼ì¸ ì´ë¦„ ë¶€ë¶„ì¼ì¹˜ í•„í„°(ì„ íƒ)"),
    domainName: Optional[str] = Query(None, description="íƒœê·¸ DomainName=... ìœ¼ë¡œ í•„í„°(ì„ íƒ)"),
    domainId: Optional[str] = Query(None, description="íƒœê·¸ DomainId=... ìœ¼ë¡œ í•„í„°(ì„ íƒ)"),
):
    try:
        region_list = _parse_regions(regions, profile)
        out: List[Dict[str, Any]] = []

        for r in region_list:
            try:
                pipes = lineage_lib.list_pipelines_with_domain(region=r, profile=profile)

                if name:
                    s = name.lower()
                    pipes = [p for p in pipes if s in p["name"].lower()]
                if domainName:
                    dn = domainName.lower()
                    pipes = [p for p in pipes
                             if (p.get("matchedDomain") and p["matchedDomain"].get("DomainName","").lower() == dn)
                             or ((p.get("tags") or {}).get("DomainName","").lower() == dn)]
                if domainId:
                    pipes = [
                        p for p in pipes
                        if (p.get("matchedDomain") and p["matchedDomain"].get("DomainId") == domainId)
                        or ((p.get("tags") or {}).get("DomainId") == domainId)
                    ]

                if includeLatestExec:
                    sess = boto3.session.Session(profile_name=profile, region_name=r) if profile \
                        else boto3.session.Session(region_name=r)
                    sm = sess.client("sagemaker", config=_BOTO_CFG)
                    for p in pipes:
                        try:
                            p["latestExecution"] = _get_latest_pipeline_execution(sm, p["name"])
                        except Exception:
                            p["latestExecution"] = {}

                out.append({"region": r, "pipelines": pipes})
            except Exception as e:
                out.append({"region": r, "error": str(e), "pipelines": []})

        return {"regions": out}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"sagemaker pipelines error: {e}")

# -----------------------------------------------------------------------------#
# 2) ë‹¨ê±´ ë¼ì¸ë¦¬ì§€
# -----------------------------------------------------------------------------#
@app.get("/lineage")
def lineage_endpoint(
    pipeline: str = Query(..., description="SageMaker Pipeline Name"),
    region: str = Query(..., description="e.g., ap-northeast-2"),
    domain: str | None = Query(None, description="Optional SageMaker DomainName tag filter"),
    includeLatestExec: bool = Query(False, description="Include latest execution info"),
    profile: str | None = Query(None, description="Local dev only; AWS profile name"),
    view: str = Query("both", regex="^(pipeline|data|both)$", description="pipeline | data | both"),
):
    try:
        data = lineage_lib.get_lineage_json(
            region=region,
            pipeline_name=pipeline,
            domain_name=domain,
            include_latest_exec=includeLatestExec,
            profile=profile,
            view=view,
        )
        return data
    except ValueError as ve:
        raise HTTPException(status_code=404, detail=str(ve))
    except ClientError as ce:
        err = ce.response.get("Error", {})
        raise HTTPException(
            status_code=502,
            detail={"type":"AWSClientError","code":err.get("Code"),"message":err.get("Message")}
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail={"type":"ServerError","message":str(e)})

# -----------------------------------------------------------------------------#
# 3) ë„ë©”ì¸ ë‚´ ëª¨ë“  íŒŒì´í”„ë¼ì¸ ë¼ì¸ë¦¬ì§€
# -----------------------------------------------------------------------------#
@app.get("/lineage/by-domain")
def lineage_by_domain(
    region: str = Query(..., description="ë¦¬ì „"),
    domain: str = Query(..., description="DomainName"),
    includeLatestExec: bool = Query(False),
    profile: str | None = Query(None),
    view: str = Query("both", regex="^(pipeline|data|both)$"),
):
    try:
        pipes = lineage_lib.list_pipelines_with_domain(region=region, profile=profile)
        targets = [
            p["name"] for p in pipes
            if (p.get("matchedDomain") and p["matchedDomain"].get("DomainName") == domain)
            or ((p.get("tags") or {}).get("DomainName") == domain)
        ]
        if not targets:
            raise HTTPException(status_code=404, detail=f"no pipelines tagged with DomainName={domain} in {region}")

        results = []
        for name in targets:
            try:
                data = lineage_lib.get_lineage_json(
                    region=region,
                    pipeline_name=name,
                    domain_name=domain,
                    include_latest_exec=includeLatestExec,
                    profile=profile,
                    view=view,
                )
                results.append({"pipeline": name, "ok": True, "data": data})
            except Exception as e:
                results.append({"pipeline": name, "ok": False, "error": str(e)})

        return {"region": region, "domain": domain, "count": len(results), "results": results}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"by-domain error: {e}")

# -----------------------------------------------------------------------------#
# 4) Dataset schema endpoints
# -----------------------------------------------------------------------------#
@app.post("/datasets/schema/scan")
def scan_dataset_schema(
    region: str = Query(..., description="e.g., ap-northeast-2"),
    s3_uri: str = Query(..., description="s3://bucket/prefix"),
    max_objects: int = Query(5, ge=1, le=50),
    max_bytes: int = Query(256*1024, ge=4096, le=5*1024*1024),
):
    """S3 prefixì—ì„œ ìƒ˜í”Œì„ ì½ì–´ JSON/CSV(+Parquet) ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ í›„ ë²„ì „ìœ¼ë¡œ ì €ì¥"""
    sch = sample_schema(region=region, s3_uri=s3_uri, max_objects=max_objects, max_bytes=max_bytes)
    b, p = parse_s3_uri(s3_uri)
    dsid = dataset_id_from_s3(b, p)
    policy = {"region": region, "s3_uri": s3_uri, "max_objects": max_objects, "max_bytes": max_bytes}
    rec = save_schema(dsid, sch, policy)
    return {"ok": True, "dataset_id": dsid, "version": rec["version"], "schema": sch}

@app.get("/datasets/{bucket}/{prefix:path}/schema")
def get_dataset_schema(bucket: str, prefix: str, version: int | None = None):
    """ì €ì¥ëœ ìŠ¤í‚¤ë§ˆ ë²„ì „ ì¡°íšŒ(ë¯¸ì§€ì • ì‹œ ìµœì‹ )"""
    dsid = dataset_id_from_s3(bucket, prefix)
    rec = get_version(dsid, version)
    if not rec:
        raise HTTPException(404, f"schema not found: {dsid}")
    return {"dataset_id": dsid, "version": rec["version"], "policy": rec["policy"], "schema": rec["schema"]}

@app.get("/datasets/{bucket}/{prefix:path}/schema/versions")
def list_dataset_schema_versions(bucket: str, prefix: str):
    dsid = dataset_id_from_s3(bucket, prefix)
    vers = list_versions(dsid)
    return [{"version": v["version"], "sampled_at": v["sampled_at"], "policy": v["policy"]} for v in vers]

# -----------------------------------------------------------------------------#
# 5) SQL ë‹¨ê±´ íŒŒì‹± í…ŒìŠ¤íŠ¸
# -----------------------------------------------------------------------------#
@app.post("/sql/lineage")
def api_sql_lineage(sql: str = Query(...), dialect: str | None = Query(None)):
    return try_parse(sql, dialect=dialect)

# -----------------------------------------------------------------------------#
# 6) Feature Store helpers
# -----------------------------------------------------------------------------#
@app.get("/featurestore/feature-groups")
def api_list_feature_groups(
    region: str,
    profile: str | None = None,
    nameContains: str | None = None,
):
    return {"items": list_feature_groups(region=region, profile=profile, name_contains=nameContains)}

@app.get("/featurestore/feature-groups/{name}")
def api_describe_feature_group(
    name: str,
    region: str,
    profile: str | None = None,
):
    return describe_feature_group(region=region, name=name, profile=profile)

# -----------------------------------------------------------------------------#
# 7) SQL ìë™ ìˆ˜ì§‘ (repo_path or git_url)
# -----------------------------------------------------------------------------#
USE_SQL_AUTOCOLLECT = os.getenv("USE_SQL_AUTOCOLLECT", "true").lower() == "true"

if USE_SQL_AUTOCOLLECT:

    @app.post("/tasks/sql/refresh")
    def refresh_sql_lineage(
        # A) ê¸°ì¡´ ë¡œì»¬ ê²½ë¡œ ë°©ì‹
        repo_path: str | None = Query(None, description="ë ˆí¬ ë£¨íŠ¸ ê²½ë¡œ"),

        # B) ì›ê²© Git ë°©ì‹
        git_url: str | None = Query(None, description="Git HTTPS URL"),
        branch: str = Query("main", description="Git branch"),
        subdir: str | None = Query(None, description="Git sub-directory (ì˜ˆ: models)"),
        token: str | None = Query(None, description="Git token/PAT (í•„ìš” ì‹œ)"),

        # ê³µí†µ
        pipeline: str = Query(...),
        job_id: str | None = Query(None),
        dialect: str | None = Query(None),
    ):
        """
        repo_path ë˜ëŠ” git_url ì¤‘ 'í•˜ë‚˜'ëŠ” í•„ìˆ˜.
        ìˆ˜ì§‘ëœ SQLì€ try_parse â†’ put() ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        """
        if (repo_path is None) and (git_url is None):
            raise HTTPException(status_code=400, detail="repo_path or git_url required")

        tmp_dir: Path | None = None
        try:
            # 1) ì†ŒìŠ¤ ê²°ì •
            if git_url:
                tmp_dir = shallow_clone(git_url=git_url, branch=branch, subdir=subdir, token=token)
                scan_root = str(tmp_dir)  # collect_from_repoëŠ” str ê²½ë¡œ ê¸°ëŒ€
            else:
                scan_root_path = Path(repo_path).expanduser().resolve()
                if not scan_root_path.exists():
                    raise HTTPException(status_code=404, detail=f"repo_path not found: {scan_root_path}")
                scan_root = str(scan_root_path)

            # 2) ìˆ˜ì§‘ â†’ íŒŒì‹± â†’ ì €ì¥
            items = collect_from_repo(scan_root)
            saved = 0
            for it in items:
                parsed = try_parse(it.get("sql", ""), dialect=dialect)
                if not parsed.get("ok"):
                    continue
                rec = {
                    "pipeline": pipeline,
                    "step": guess_step_from_path(it.get("file", ""), pipeline),
                    "job_id": job_id,
                    "file": it.get("file"),
                    "sql": it.get("sql"),
                    "parsed": parsed,
                }
                put(rec)
                saved += 1

            return {"ok": True, "saved": saved, "pipeline": pipeline}

        finally:
            # 3) ì„ì‹œ ê¹ƒ í´ë¡  ë””ë ‰í„°ë¦¬ ì •ë¦¬
            if tmp_dir and tmp_dir.exists():
                shutil.rmtree(tmp_dir, ignore_errors=True)

    @app.get("/jobs/{job_id}/sql-lineage")
    def sql_lineage_by_job(job_id: str):
        return {"ok": True, "job_id": job_id, "items": get_by_job(job_id)}

    @app.get("/pipelines/{name}/sql-lineage")
    def sql_lineage_by_pipeline(name: str):
        rows = get_by_pipeline(name)
        latest: dict[str, dict] = {}
        for r in rows:
            step = r.get("step")
            cur = latest.get(step)
            if (not cur) or r.get("ts", 0) > cur.get("ts", 0):
                latest[step] = r
        summary = []
        for step, r in latest.items():
            p = r.get("parsed", {})
            summary.append({
                "step": step,
                "dst": p.get("dst"),
                "sources": p.get("sources", []),
                "columns": p.get("columns", []),
                "file": r.get("file"),
                "ts": r.get("ts"),
            })
        return {"ok": True, "pipeline": name, "steps": summary}

# -----------------------------------------------------------------------------#
# 8) ë ˆí¬ ì—†ì´ ë°”ë¡œ ì²´í—˜: Inline SQL íŒŒì‹± ì €ì¥
# -----------------------------------------------------------------------------#
class InlineSqlReq(BaseModel):
    pipeline: str
    sql: str | None = None
    sql_list: List[str] | None = None
    job_id: str | None = None
    dialect: str | None = None

@app.post("/tasks/sql/inline", summary="Parse & Store Inline SQL (no repo needed)")
def task_sql_inline(req: InlineSqlReq):
    payload: List[str] = []
    if req.sql:
        payload.append(req.sql)
    if req.sql_list:
        payload.extend(req.sql_list)
    if not payload:
        raise HTTPException(status_code=400, detail="sql or sql_list required")

    saved = 0
    for i, sql in enumerate(payload, start=1):
        parsed = try_parse(sql, dialect=req.dialect)
        if not parsed.get("ok"):
            continue
        put({
            "pipeline": req.pipeline,
            "job_id": req.job_id,
            "step": f"inline::{i}",
            "file": f"inline::{i}",
            "sql": sql,
            "parsed": parsed
        })
        saved += 1
    return {"ok": True, "saved": saved, "pipeline": req.pipeline}

@app.get("/lineage/schema", summary="Get Lineage Schema")
async def get_lineage_schema(
    request: Request,
    pipeline: str = Query(..., description="SageMaker Pipeline name"),
    region: str = Query("ap-northeast-2"),
    include_featurestore: bool = Query(True),
    include_sql: bool = Query(True),
    scan_if_missing: bool = Query(False, description="trueë©´ ìŠ¤í‚¤ë§ˆ ì—†ì„ ë•Œ ìë™ ìŠ¤ìº” ì‹œë„"),
):
    try:
        return await fetch_schema_layer(
            request=request,
            pipeline=pipeline,
            region=region,
            include_featurestore=include_featurestore,
            include_sql=include_sql,
            scan_if_missing=scan_if_missing,
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"schema layer error: {e}")
    
@app.get("/schema", summary="Get Schema (Alias)")
async def get_schema_alias(
    request: Request,
    pipeline: str = Query(...),
    region: str = Query("ap-northeast-2"),
    include_featurestore: bool = Query(True),
    include_sql: bool = Query(True),
    scan_if_missing: bool = Query(False),
):
    """
    /schema ì—”ë“œí¬ì¸íŠ¸ (í”„ë¡ íŠ¸ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­)
    """
    return await fetch_schema_layer(
        request=request,
        pipeline=pipeline,
        region=region,
        include_featurestore=include_featurestore,
        include_sql=include_sql,
        scan_if_missing=scan_if_missing,
    )

# -----------------------------------------------------------------------------#
# Entrypoint
# -----------------------------------------------------------------------------#
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8300)
